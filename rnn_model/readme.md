# Sequence modeling of traffic using 1-to-1 LSTM architecture

The module trains a simple 1 layer LSTM model from a sequence of traffic. The input data is padded to the maximum sequence length found in the data and normalized based on the user-specified method, which will be described below. A zero-vector is then appended to the start of every sequence, such that we can predict the first _real_ element in the sequence. The model output is generated by applying a one timestep lag (e.g. __input__: X<sub>t</sub> -> __output__ X<sub>t+1</sub>), where the aim is to predict the next element in the sequence given the current element. A train-test-split is applied (default: 30%) and the training dataset is used for model training, while the test dataset is used for evaluation. The loss function used is Mean Squared Error, while the evaluation metric used is Cosine Similarity. Lastly, a series of visualization is plotted.

## Getting Started

With a Python 3 environment, install the dependencies from `requirements.txt`

```
pip install -r requirements.txt
```

Execute the following possible commands:

_Basic_
```
python main.py -f 'features.csv'
```
_Complete_
```
python main.py -f 'features.csv' -n 1 -e 100 -s 'results/expt999'
```

Required arguments:
* -f: Directory of feature file to be used for model training

Optional arguments:
* -n : Normalization options for features (default=1)
* -e : Number of epoch for training (default=100)
* -s : Flag for saving visualization plots. If not specified, the plots will be displayed 
* -m : Directory of existing model to be used for training

Normalization options:
1. Normalize each sample independently into unit vectors
2. Standardize each feature across ALL traffic. [Source](http://cs231n.github.io/neural-networks-2/)
3. Scale each feature between min and max of feature

## Loading dataset for model training

A step-by-step guide to loading the data into memory _nicely_. Typically, we can load the whole dataset into memory if the dataset is small, but the dataset we are dealing with is too big (â‰¥1GB), hence we can use `keras.utils.Sequence()` class to load the dataset in batches for memory efficiency, much like how Python Generators work. These are the steps to follow:
1. Import `utils_datagen` from the folder rnn_model, which contains utility functions
```
import utils_datagen
```
2. Set `path_to_feature_file` to the directory path of the feature file to be used and use the `get_mmapdata_and_byteoffset` function to obtain the mmap object and the byte offsets of the lines in the feature file. You can refer to [this](https://stackoverflow.com/questions/24492331/shuffle-a-large-list-of-items-without-loading-in-memory) for more info on mmap and how byte offsets are generated
```
mmap_data, byte_offset = utils_datagen.get_mmapdata_and_byteoffset(path_to_feature_file)
```
3. There are 2 options for normalization: L2 normalization or Min-Max normalization. For min-max normalization, the lower and upper bound must first be discovered using the function `utils_datagen.get_min_max`
```
# Option 1: L2 Normalization
norm_fn = utils_datagen.normalize(1)
# Option 2: Min-Max Normalization
min_max_feature = utils_datagen.get_min_max(mmap_data, byte_offset)
norm_fn = utils_datagen.normalize(2, min_max_feature)
```
4. Initialize the generator with the following parameters. `batch_size` refers to the number of training observation for 1 batch (default=64). `sequence_len` refers to the number of packets in a training observation (default=100). The rest are derived from above.
```
data_generator = utils_datagen.BatchGenerator(mmap_data, byte_offset, batch_size, sequence_len, norm_fn)
```

## Visualization plots

__Plotting the prediction on sequence length at every epoch.__ Motivation: to determine if the model is learning and is able to model after the traffic data

At epoch 1

![plot1](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/train_predict_pktlen2%20(nicer%20diagram)/epoch1.png)

At epoch 10

![plot2](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/train_predict_pktlen2%20(nicer%20diagram)/epoch10.png)

At epoch 20

![plot3](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/train_predict_pktlen2%20(nicer%20diagram)/epoch20.png)

__Plotting the cosine similarity for True packets__

![plot4](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/acc_dist_truepkts_.png)

__Plotting the mean and median cosine similarity over epoch and final cosine similarity for #1 packet__

![plot5](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/expt1/acc_dist_1pkts.png)

__Plotting the mean and median cosine similarity over epoch and final cosine similarity for first \_\_ packets__

First 10 packets
![plot6](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/expt1/acc_dist_10pkts.png)

First 30 packets
![plot7](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/expt1/acc_dist_30pkts.png)

First 60 packets
![plot8](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/expt1/acc_dist_60pkts.png)

First 90 packets
![plot9](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/expt1/acc_dist_90pkts.png)

__Plotting the training and validation loss__

![plot10](https://github.com/llmhyy/tls_atack/blob/master/rnn_model/results/expt1/loss.png)