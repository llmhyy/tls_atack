# Sequence modeling of traffic using 1-to-1 LSTM architecture

The module trains a simple 1 layer LSTM model from a sequence of traffic. The input data is padded to the maximum sequence length found in the data and normalized based on the user-specified method, which will be described below. A zero-vector is then appended to the start of every sequence, such that we can predict the first _real_ element in the sequence. The model output is generated by applying a one timestep lag (e.g. __input__: X<sub>t</sub> -> __output__ X<sub>t+1</sub>), where the aim is to predict the next element in the sequence given the current element. A train-test-split is applied (default: 30%) and the training dataset is used for model training, while the test dataset is used for evaluation. The loss function used is Mean Squared Error, while the evaluation metric used is Cosine Similarity. Lastly, a series of visualization is plotted.

## Getting Started

With a Python 3 environment, install the dependencies from `requirements.txt`
```
pip install -r requirements.txt
```
For training a enw model, this is a sample code:
```
python train_rnn.py -e 100 -t ../new_traffic -f ../new_traffic/extracted_features/features_tls_2019-03-01_11-06-12.csv 
```
For re-training an existing model, this is a sample code
```
python train_rnn.py -e 100 -t ../new_traffic -f ../new_traffic/extracted_features/features_tls_2019-03-01_11-06-12.csv -m ../new_traffic/trained_rnn/expt_2019-03-09_22-22-35/model/rnnmodel_2019-03-09_22-22-35.h5
```

Required arguments:
* -t: Input top-level directory of the traffic module containing extracted features
* -f: Input directory path of feature file to be used

Optional arguments:
* -e : Number of epoch for training (default=100)
* -s : Flag for saving visualization plots. If not specified, the plots will be displayed (default=True)
* -m : Input directory of existing model to be used for training

Normalization options:
1. Normalize each sample independently into unit vectors
2. Standardize each feature across ALL traffic. [Source](http://cs231n.github.io/neural-networks-2/)
3. Scale each feature between min and max of feature

## Loading dataset for model training

A step-by-step guide to loading the data into memory _nicely_. Typically, we can load the whole dataset into memory if the dataset is small, but the dataset we are dealing with is too big (â‰¥1GB), hence we can use `keras.utils.Sequence()` class to load the dataset in batches for memory efficiency, much like how Python Generators work. These are the steps to follow:
1. Import `utils_datagen` from the folder rnn_model, which contains utility functions
```
import utils_datagen
```
2. Set `path_to_feature_file` to the directory path of the feature file to be used and use the `get_mmapdata_and_byteoffset` function to obtain the mmap object and the byte offsets of the lines in the feature file. You can refer to [this](https://stackoverflow.com/questions/24492331/shuffle-a-large-list-of-items-without-loading-in-memory) for more info on mmap and how byte offsets are generated
```
mmap_data, byte_offset = utils_datagen.get_mmapdata_and_byteoffset(path_to_feature_file)
```
3. There are 2 options for normalization: L2 normalization or Min-Max normalization. For min-max normalization, the lower and upper bound must first be discovered using the function `utils_datagen.get_min_max`
```
# Option 1: L2 Normalization
norm_fn = utils_datagen.normalize(1)
# Option 2: Min-Max Normalization
min_max_feature = utils_datagen.get_min_max(mmap_data, byte_offset)
norm_fn = utils_datagen.normalize(2, min_max_feature)
```
4. Initialize the generator with the following parameters. `batch_size` refers to the number of training observation for 1 batch (default=64). `sequence_len` refers to the number of packets in a training observation (default=100). The rest are derived from above.
```
data_generator = utils_datagen.BatchGenerator(mmap_data, byte_offset, batch_size, sequence_len, norm_fn)
```
5. The data generator will generate batches in this format: (input, target). Both input and target will have the shape: (batch_size, sequence_len, 146). Note that the size of the third dimension is 146 because there are a total of 146 extracted features in a packet. To use the generator, you can do something like this:
```
for (batch input, batch target) in data_generator:
	# your code to manipulate batch input and batch target...
```

## Visualization plots

__Plotting the prediction on sequence length at every epoch.__ Motivation: to determine if the model is learning and is able to model after the traffic data

__Plotting the cosine similarity for True packets__

__Plotting the mean and median cosine similarity over epoch and final cosine similarity for #1 packet__

__Plotting the mean and median cosine similarity over epoch and final cosine similarity for first \_\_ packets__

__Plotting the training and validation loss__
